{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "from typing import Any, Callable, Sequence, Optional\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Dense(features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        bias: (5,),\n",
       "        kernel: (10, 5),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (10,)) # Dummy input\n",
    "params = model.init(key2, x) # Initialization call\n",
    "jax.tree_map(lambda x: x.shape, params) # Checking output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.6105604   0.03385275  1.0863333  -1.480299    0.48895663  1.0625157\n",
      "  0.5417483   0.01702273  0.27226844  0.3052244 ]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.7358944,  1.3583755, -0.7976872,  0.8168598,  0.6297792],            dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (20, 10) ; y shape: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "# Set problem dimensions\n",
    "nsamples = 20\n",
    "xdim = 10\n",
    "ydim = 5\n",
    "\n",
    "# Generate random ground truth W and b\n",
    "key = random.PRNGKey(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (xdim, ydim))\n",
    "b = random.normal(k2, (ydim,))\n",
    "true_params = freeze({'params': {'bias': b, 'kernel': W}})\n",
    "\n",
    "# Generate samples with additional noise\n",
    "ksample, knoise = random.split(k1)\n",
    "x_samples = random.normal(ksample, (nsamples, xdim))\n",
    "y_samples = jnp.dot(x_samples, W) + b\n",
    "y_samples += 0.1*random.normal(knoise,(nsamples, ydim)) # Adding noise\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2215654  -1.3890593  -1.1903723  -1.3718774   0.33591986  1.2989264\n",
      "  -0.798972    0.30542666  0.2626647   0.16920286]\n",
      " [ 1.3967623   0.33278397 -0.02423434  0.7279588  -0.02220635 -0.04626204\n",
      "   1.4736844  -0.8445644   0.35538498  0.92975104]]\n"
     ]
    }
   ],
   "source": [
    "print(x_samples[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44524443 -4.254266   -2.085645   -0.2375911  -0.5508175 ]\n",
      " [-2.023773    2.1660166   3.138679   -1.5070897   2.0076575 ]]\n"
     ]
    }
   ],
   "source": [
    "print(y_samples[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a MSE Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mse_func(x_batched, y_batched):\n",
    "    def mse(params):\n",
    "    # Define the squared loss for a single pair (x,y)\n",
    "        def squared_error(x, y):\n",
    "            pred = model.apply(params, x)\n",
    "            return jnp.inner(y-pred, y-pred)/2.0\n",
    "        # We vectorize the previous to compute the average of the loss on all samples.\n",
    "        return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
    "    return jax.jit(mse) # And finally we jit the result.\n",
    "\n",
    "# Get the sampled loss\n",
    "loss = make_mse_func(x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.023639778\n",
      "Loss step 0:  38.094772\n",
      "Loss step 10:  0.44692174\n",
      "Loss step 20:  0.10053458\n",
      "Loss step 30:  0.03582275\n",
      "Loss step 40:  0.018846864\n",
      "Loss step 50:  0.013864852\n",
      "Loss step 60:  0.012312567\n",
      "Loss step 70:  0.011812925\n",
      "Loss step 80:  0.01164931\n",
      "Loss step 90:  0.011595252\n",
      "Loss step 100:  0.011577307\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.3 # Gradient step size\n",
    "print('Loss for \"true\" W,b: ', loss(true_params))\n",
    "grad_fn = jax.value_and_grad(loss)\n",
    "\n",
    "for i in range(101):\n",
    "    # We perform one gradient update\n",
    "    loss_val, grads = grad_fn(params)\n",
    "    params = jax.tree_multimap(lambda p, g: p - alpha * g, params, grads)\n",
    "    if i % 10 == 0:\n",
    "        print('Loss step {}: '.format(i), loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.sgd(learning_rate=alpha)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  0.011576376\n",
      "Loss step 10:  0.011571027\n",
      "Loss step 20:  0.011569239\n",
      "Loss step 30:  0.011568645\n",
      "Loss step 40:  0.011568449\n",
      "Loss step 50:  0.0115683805\n",
      "Loss step 60:  0.011568364\n",
      "Loss step 70:  0.011568367\n",
      "Loss step 80:  0.011568349\n",
      "Loss step 90:  0.011568355\n",
      "Loss step 100:  0.011568355\n"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "    loss_val, grads = loss_grad_fn(params)\n",
    "    updates, opt_state = tx.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 10 == 0:\n",
    "        print('Loss step {}: '.format(i), loss_val)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52d10dda8dd4acbc0136d307f056abb067e83eda640decd801853dd83bdd7356"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('jax': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
